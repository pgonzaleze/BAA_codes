{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier using bleaching alert area (BAA) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "#from scipy.stats import shapiro, levene, bartlett, kruskal\n",
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "# Check scikit-learn version\n",
    "from sklearn import __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load full dataset\n",
    "'''\n",
    "data = pd.read_csv('baa.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Subset DF by SEVERITY_CODE [0,1,2,3]\n",
    "'''\n",
    "#data = data.dropna() # drop rows that contains NaN's \n",
    "data = data[(data.SEVERITY_CODE == 0)|(data.SEVERITY_CODE == 1)|(data.SEVERITY_CODE == 2)|(data.SEVERITY_CODE == 3)] \n",
    "#data = data[(data.YEAR >= 2015) & (data.YEAR <= 2016)] # subset a single event\n",
    "#data = data[(data.YEAR >= 1997) & (data.YEAR <= 1998)] \n",
    "#list(data.columns)\n",
    "data = data.dropna() # drop rows that contains NaN's\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Editable variable's model \n",
    "'''\n",
    "## \n",
    "X1=data[['Cases']] # Features (dependent variable)\n",
    "y=data['SEVERITY_CODE'] #labels (indipendent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Variance inflation factor VIF\n",
    "'''\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Get variables for which to compute VIF and add intercept term\n",
    "X1['Intercept'] = 1\n",
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X1.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])]\n",
    "# View results using print\n",
    "print(vif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Build the models\n",
    "'''\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=10)\n",
    "model.fit(X1,y)\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=10)\n",
    "n_scores = cross_val_score(model, X1, y, cv=cv) #n_jobs=-1, error_score='raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Report performance\n",
    "'''\n",
    "print('Cross val score: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "print('\\n')\n",
    "# Features importance\n",
    "print('=== features importances ===')\n",
    "fi = pd.DataFrame({'feature': list(X1.columns),\n",
    "                   'importance': model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Confusion matrix\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "y_pred=model.predict(X1)\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "conf_mat_norm = confusion_matrix(y, y_pred,normalize='all')\n",
    "print(\"=== Confusion matrix ===\")\n",
    "print(conf_mat)\n",
    "print('\\n')\n",
    "print(\"=== Confusion matrix normalized ===\")\n",
    "print(conf_mat_norm)\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y, y_pred))\n",
    "print('\\n')\n",
    "print('=== Accuracy and Kappa ===')\n",
    "print('accuracy', metrics.accuracy_score(y, y_pred))\n",
    "print('\\n')\n",
    "print('kappa', metrics.cohen_kappa_score(y, y_pred))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Evaluation between classifications models through \"log loss\"\n",
    "'''\n",
    "model_probs = model.predict_proba(X1)\n",
    "score = log_loss(y, model_probs)\n",
    "\n",
    "\n",
    "'''\n",
    "    Evaluation between classifications models through \"ROC_AUC\"\n",
    "'''\n",
    "roc_value = roc_auc_score(y, model_probs, multi_class='ovo') # ovo': Computes the average AUC of all possible pairwise combinations of classes\n",
    "\n",
    "print('=== roc_auc_score ===') \n",
    "print(roc_value)\n",
    "print(' ')\n",
    "print('=== log_loss_score ===') \n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "#Heat map with annot=True to annotate cells\n",
    "sb.heatmap(conf_mat, annot=True, ax = ax, fmt='d', cmap='Blues') # actual cases\n",
    "#sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, ax = ax, fmt='.2%', cmap='Blues') #percentage\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted bleaching level');ax.set_ylabel('True bleaching level'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3']); ax.yaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3'])\n",
    "plt.show()\n",
    "#plt.savefig('DHW_monmean_CF_a_monmean.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pd.DataFrame({'Real':y, 'Predictions':y_pred})\n",
    "#comparisons.to_csv('comparisons.csv')\n",
    "print(comparisons[['Real','Predictions']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
